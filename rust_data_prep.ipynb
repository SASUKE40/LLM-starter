{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "t4ce0s9oevi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c68a1577d6d40149c8fa037171ebc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3b9f1541784f99a2fc98ca74a17afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rust samples: 1386585\n",
      "Columns: ['hexsha', 'size', 'ext', 'lang', 'max_stars_repo_path', 'max_stars_repo_name', 'max_stars_repo_head_hexsha', 'max_stars_repo_licenses', 'max_stars_count', 'max_stars_repo_stars_event_min_datetime', 'max_stars_repo_stars_event_max_datetime', 'max_issues_repo_path', 'max_issues_repo_name', 'max_issues_repo_head_hexsha', 'max_issues_repo_licenses', 'max_issues_count', 'max_issues_repo_issues_event_min_datetime', 'max_issues_repo_issues_event_max_datetime', 'max_forks_repo_path', 'max_forks_repo_name', 'max_forks_repo_head_hexsha', 'max_forks_repo_licenses', 'max_forks_count', 'max_forks_repo_forks_event_min_datetime', 'max_forks_repo_forks_event_max_datetime', 'content', 'avg_line_length', 'max_line_length', 'alphanum_fraction']\n",
      "Dataset({\n",
      "    features: ['hexsha', 'size', 'ext', 'lang', 'max_stars_repo_path', 'max_stars_repo_name', 'max_stars_repo_head_hexsha', 'max_stars_repo_licenses', 'max_stars_count', 'max_stars_repo_stars_event_min_datetime', 'max_stars_repo_stars_event_max_datetime', 'max_issues_repo_path', 'max_issues_repo_name', 'max_issues_repo_head_hexsha', 'max_issues_repo_licenses', 'max_issues_count', 'max_issues_repo_issues_event_min_datetime', 'max_issues_repo_issues_event_max_datetime', 'max_forks_repo_path', 'max_forks_repo_name', 'max_forks_repo_head_hexsha', 'max_forks_repo_licenses', 'max_forks_count', 'max_forks_repo_forks_event_min_datetime', 'max_forks_repo_forks_event_max_datetime', 'content', 'avg_line_length', 'max_line_length', 'alphanum_fraction'],\n",
      "    num_rows: 1386585\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Download the full Rust subset to disk (non-streaming)\n",
    "# This will download all Rust files — may take a while depending on size\n",
    "rust_ds = load_dataset(\n",
    "    \"bigcode/the-stack-dedup\",\n",
    "    data_dir=\"data/rust\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "print(f\"Total Rust samples: {len(rust_ds)}\")\n",
    "print(f\"Columns: {rust_ds.column_names}\")\n",
    "print(rust_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7k9inwpvfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from tokenizers) (1.4.1)\n",
      "Requirement already satisfied: filelock in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.3)\n",
      "Requirement already satisfied: typer-slim in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.16.0)\n",
      "Requirement already satisfied: typer>=0.23.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.23.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/edward/.pyenv/versions/3.11.14/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.1.2)\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.22.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gsocirgom4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix torch_shm_manager permission issue (required for streaming datasets with torch installed)\n",
    "import subprocess, site, os\n",
    "shm_path = os.path.join(site.getsitepackages()[0], \"torch\", \"bin\", \"torch_shm_manager\")\n",
    "if os.path.exists(shm_path):\n",
    "    subprocess.run([\"chmod\", \"+x\", shm_path], check=True)\n",
    "    print(f\"Fixed permissions on {shm_path}\")\n",
    "else:\n",
    "    print(\"torch_shm_manager not found — skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "vxzh5wkskqe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d746e61c9b41949edb8399d2c77525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cffce6fc55b428191a274fe9bb9f02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['hexsha', 'size', 'ext', 'lang', 'max_stars_repo_path', 'max_stars_repo_name', 'max_stars_repo_head_hexsha', 'max_stars_repo_licenses', 'max_stars_count', 'max_stars_repo_stars_event_min_datetime', 'max_stars_repo_stars_event_max_datetime', 'max_issues_repo_path', 'max_issues_repo_name', 'max_issues_repo_head_hexsha', 'max_issues_repo_licenses', 'max_issues_count', 'max_issues_repo_issues_event_min_datetime', 'max_issues_repo_issues_event_max_datetime', 'max_forks_repo_path', 'max_forks_repo_name', 'max_forks_repo_head_hexsha', 'max_forks_repo_licenses', 'max_forks_count', 'max_forks_repo_forks_event_min_datetime', 'max_forks_repo_forks_event_max_datetime', 'content', 'avg_line_length', 'max_line_length', 'alphanum_fraction']\n",
      "Language: Rust\n",
      "Size: 4965 bytes\n",
      "Content preview:\n",
      "use crate::interactive::{\n",
      "    widgets::{\n",
      "        Entries, EntriesProps, Footer, FooterProps, Header, HelpPane, HelpPaneProps, MarkPane,\n",
      "        MarkPaneProps, COLOR_MARKED,\n",
      "    },\n",
      "    AppState, DisplayOptions, FocussedPane,\n",
      "};\n",
      "use dua::traverse::Traversal;\n",
      "use std::borrow::Borrow;\n",
      "use tui::{\n",
      "    buffer::Buffer,\n",
      "    layout::{Constraint, Direction, Layout, Rect},\n",
      "    style::Modifier,\n",
      "    style::{Color, Style},\n",
      "};\n",
      "use Constraint::*;\n",
      "use FocussedPane::*;\n",
      "\n",
      "pub struct MainWindowProps<'a> {\n",
      "    pub tra\n"
     ]
    }
   ],
   "source": [
    "# the-stack-v2-dedup only has metadata (no source code content).\n",
    "# Switch to the-stack-dedup (v1) which includes the actual code in a \"content\" column.\n",
    "# NOTE: This is a gated dataset — accept terms at https://huggingface.co/datasets/bigcode/the-stack-dedup\n",
    "from datasets import load_dataset\n",
    "\n",
    "rust_code_ds = load_dataset(\n",
    "    \"bigcode/the-stack-dedup\",\n",
    "    data_dir=\"data/rust\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# Preview first sample\n",
    "sample = next(iter(rust_code_ds))\n",
    "print(f\"Columns: {list(sample.keys())}\")\n",
    "print(f\"Language: {sample['lang']}\")\n",
    "print(f\"Size: {sample['size']} bytes\")\n",
    "print(f\"Content preview:\\n{sample['content'][:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3q3gqh15wdx",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5ef9e786eb45e8855ee9433646bcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Train a BPE tokenizer on a subset of Rust code\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "\n",
    "# Collect training corpus by streaming a subset\n",
    "TRAIN_SAMPLES = 50_000  # number of files to use for tokenizer training\n",
    "\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    batch = []\n",
    "    for i, sample in enumerate(dataset):\n",
    "        if i >= TRAIN_SAMPLES:\n",
    "            break\n",
    "        batch.append(sample[\"content\"])\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "# Initialize a BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Configure the trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32_000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<|endoftext|>\", \"<|padding|>\"],\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Re-create the streaming dataset for training\n",
    "train_stream = load_dataset(\n",
    "    \"bigcode/the-stack-dedup\",\n",
    "    data_dir=\"data/rust\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(batch_iterator(train_stream), trainer=trainer)\n",
    "print(f\"Vocab size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "u3qmz53im8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (40): [648, 1885, 241, 239, 227, 268, 399, 527, 27, 659, 29, 74, 362, 31, 238, 659, 225, 355, 356, 227, 527, 15, 794, 9, 2351, 261, 227, 1109, 510, 5761]...\n",
      "Decoded: ['fn', 'Ġmain', '()', 'Ġ{', 'ĊĠĠĠ', 'Ġlet', 'Ġmut', 'Ġv', ':', 'ĠVec', '<', 'i', '32', '>', 'Ġ=', 'ĠVec', '::', 'new', '();', 'ĊĠĠĠ', 'Ġv', '.', 'push', '(', '42', ');', 'ĊĠĠĠ', 'Ġprintln', '!(\"', 'Hello']...\n",
      "\n",
      "Round-trip decoded:\n",
      "fn main() {\n",
      "    let mut v: Vec<i32> = Vec::new();\n",
      "    v.push(42);\n",
      "    println!(\"Hello, Rust! {}\", v[0]);\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer on a Rust code snippet\n",
    "test_code = \"\"\"fn main() {\n",
    "    let mut v: Vec<i32> = Vec::new();\n",
    "    v.push(42);\n",
    "    println!(\"Hello, Rust! {}\", v[0]);\n",
    "}\"\"\"\n",
    "\n",
    "encoded = tokenizer.encode(test_code)\n",
    "print(f\"Tokens ({len(encoded.ids)}): {encoded.ids[:30]}...\")\n",
    "print(f\"Decoded: {encoded.tokens[:30]}...\")\n",
    "\n",
    "# Verify round-trip\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(f\"\\nRound-trip decoded:\\n{decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v8b8jjvexn",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import json, html as html_mod\n",
    "\n",
    "# --- Tiktokenizer-style interactive visualizer ---\n",
    "COLORS = [\n",
    "    \"#bae6fd\", \"#fde68a\", \"#bfdbfe\", \"#bbf7d0\", \"#fed7aa\",\n",
    "    \"#a5f3fc\", \"#e5e7eb\", \"#e9d5ff\", \"#c7d2fe\", \"#d9f99d\",\n",
    "    \"#fecdd3\", \"#ddd6fe\", \"#fef08a\", \"#a7f3d0\", \"#e4e4e7\",\n",
    "    \"#fecaca\", \"#f5d0fe\", \"#fbcfe8\", \"#99f6e4\",\n",
    "]\n",
    "\n",
    "def visualize_tokens(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    tokens = encoded.tokens\n",
    "    ids = encoded.ids\n",
    "    offsets = encoded.offsets\n",
    "\n",
    "    # Build colored spans HTML\n",
    "    spans_html = []\n",
    "    ids_html = []\n",
    "    for i, (tok, tid, (start, end)) in enumerate(zip(tokens, ids, offsets)):\n",
    "        color = COLORS[i % len(COLORS)]\n",
    "        # Decode the displayed text from offsets for accuracy\n",
    "        display_text = text[start:end] if end > start else tok\n",
    "        escaped = html_mod.escape(display_text)\n",
    "        # Show whitespace visually\n",
    "        escaped = escaped.replace(\" \", '<span style=\"opacity:0.5\">\\u00b7</span>')\n",
    "        escaped = escaped.replace(\"\\n\", '<span style=\"opacity:0.5\">\\\\n</span>\\n')\n",
    "        escaped = escaped.replace(\"\\t\", '<span style=\"opacity:0.5\">\\\\t</span>')\n",
    "        spans_html.append(\n",
    "            f'<span class=\"tok\" data-i=\"{i}\" style=\"background:{color};'\n",
    "            f'padding:1px 0;border-radius:3px;cursor:pointer;\"'\n",
    "            f' onmouseenter=\"hlTok({i})\" onmouseleave=\"hlTok(-1)\">'\n",
    "            f'{escaped}</span>'\n",
    "        )\n",
    "        ids_html.append(\n",
    "            f'<span class=\"tid\" data-i=\"{i}\" style=\"background:{color};'\n",
    "            f'padding:1px 4px;border-radius:3px;margin:1px;display:inline-block;'\n",
    "            f'font-size:12px;cursor:pointer;\"'\n",
    "            f' onmouseenter=\"hlTok({i})\" onmouseleave=\"hlTok(-1)\">'\n",
    "            f'{tid}</span>'\n",
    "        )\n",
    "\n",
    "    page = f\"\"\"\n",
    "    <style>\n",
    "      .tv-box {{ border:1px solid #e2e8f0; border-radius:8px; padding:16px;\n",
    "                 font-family:ui-monospace,monospace; background:#f8fafc; margin:4px 0;\n",
    "                 min-height:100px; white-space:pre-wrap; word-break:break-all;\n",
    "                 line-height:1.8; }}\n",
    "      .tv-label {{ font-weight:600; font-size:13px; color:#64748b; margin-bottom:6px; }}\n",
    "      .tok.hl, .tid.hl {{ outline:2px solid #334155; z-index:1; position:relative; }}\n",
    "    </style>\n",
    "    <div style=\"font-family:system-ui,sans-serif; max-width:900px;\">\n",
    "      <div style=\"display:flex; align-items:center; gap:16px; margin-bottom:12px;\">\n",
    "        <span style=\"font-size:18px; font-weight:700;\">Rust Tokenizer Visualizer</span>\n",
    "        <span style=\"background:#e2e8f0; padding:4px 12px; border-radius:6px;\n",
    "               font-size:14px; font-weight:600;\">{len(ids)} tokens</span>\n",
    "        <span style=\"background:#e2e8f0; padding:4px 12px; border-radius:6px;\n",
    "               font-size:14px; font-weight:600;\">{len(text)} chars</span>\n",
    "      </div>\n",
    "      <div class=\"tv-label\">Text</div>\n",
    "      <div class=\"tv-box\">{''.join(spans_html)}</div>\n",
    "      <div class=\"tv-label\" style=\"margin-top:12px;\">Token IDs</div>\n",
    "      <div class=\"tv-box\" style=\"line-height:2.2;\">{''.join(ids_html)}</div>\n",
    "    </div>\n",
    "    <script>\n",
    "    function hlTok(idx) {{\n",
    "      document.querySelectorAll('.tok,.tid').forEach(el => {{\n",
    "        if (idx >= 0 && el.dataset.i == idx) el.classList.add('hl');\n",
    "        else el.classList.remove('hl');\n",
    "      }});\n",
    "    }}\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    display(HTML(page))\n",
    "\n",
    "# --- Visualize the Rust test snippet ---\n",
    "test_code = \"\"\"fn main() {\n",
    "    let mut v: Vec<i32> = Vec::new();\n",
    "    v.push(42);\n",
    "    println!(\"Hello, Rust! {}\", v[0]);\n",
    "}\"\"\"\n",
    "\n",
    "visualize_tokens(test_code, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nih7raaiv6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9328be26971e4a50bf44b8efca87acb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='fn main() {\\n    let mut v: Vec<i32> = Vec::new();\\n    v.push(42);\\n    println!(\"Hello, Rust…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49588bd7f33400b839cf4ec77b71e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive version — type any Rust code and see live tokenization\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import html as html_mod\n",
    "\n",
    "COLORS = [\n",
    "    \"#bae6fd\", \"#fde68a\", \"#bfdbfe\", \"#bbf7d0\", \"#fed7aa\",\n",
    "    \"#a5f3fc\", \"#e5e7eb\", \"#e9d5ff\", \"#c7d2fe\", \"#d9f99d\",\n",
    "    \"#fecdd3\", \"#ddd6fe\", \"#fef08a\", \"#a7f3d0\", \"#e4e4e7\",\n",
    "    \"#fecaca\", \"#f5d0fe\", \"#fbcfe8\", \"#99f6e4\",\n",
    "]\n",
    "\n",
    "def render_tokens(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    tokens, ids, offsets = encoded.tokens, encoded.ids, encoded.offsets\n",
    "\n",
    "    spans_html, ids_html = [], []\n",
    "    for i, (tok, tid, (start, end)) in enumerate(zip(tokens, ids, offsets)):\n",
    "        color = COLORS[i % len(COLORS)]\n",
    "        display_text = text[start:end] if end > start else tok\n",
    "        escaped = html_mod.escape(display_text)\n",
    "        escaped = escaped.replace(\" \", '<span style=\"opacity:0.4\">\\u00b7</span>')\n",
    "        escaped = escaped.replace(\"\\n\", '<span style=\"opacity:0.4\">\\\\n</span>\\n')\n",
    "        escaped = escaped.replace(\"\\t\", '<span style=\"opacity:0.4\">\\\\t</span>')\n",
    "        spans_html.append(\n",
    "            f'<span class=\"tok\" data-i=\"{i}\" style=\"background:{color};'\n",
    "            f'padding:1px 0;border-radius:3px;cursor:pointer;\"'\n",
    "            f' onmouseenter=\"hlTok({i})\" onmouseleave=\"hlTok(-1)\">{escaped}</span>'\n",
    "        )\n",
    "        ids_html.append(\n",
    "            f'<span class=\"tid\" data-i=\"{i}\" style=\"background:{color};'\n",
    "            f'padding:1px 4px;border-radius:3px;margin:1px;display:inline-block;'\n",
    "            f'font-size:12px;cursor:pointer;\"'\n",
    "            f' onmouseenter=\"hlTok({i})\" onmouseleave=\"hlTok(-1)\">{tid}</span>'\n",
    "        )\n",
    "\n",
    "    return f\"\"\"\n",
    "    <style>\n",
    "      .tv-box {{ border:1px solid #e2e8f0; border-radius:8px; padding:16px;\n",
    "                 font-family:ui-monospace,monospace; background:#f8fafc; margin:4px 0;\n",
    "                 min-height:60px; white-space:pre-wrap; word-break:break-all;\n",
    "                 line-height:1.8; }}\n",
    "      .tv-label {{ font-weight:600; font-size:13px; color:#64748b; margin-bottom:6px; }}\n",
    "      .tok.hl, .tid.hl {{ outline:2px solid #334155; z-index:1; position:relative; }}\n",
    "    </style>\n",
    "    <div style=\"font-family:system-ui,sans-serif;\">\n",
    "      <div style=\"display:flex; align-items:center; gap:12px; margin-bottom:10px;\">\n",
    "        <span style=\"font-size:16px; font-weight:700;\">Rust Tokenizer Visualizer</span>\n",
    "        <span style=\"background:#dbeafe; padding:3px 10px; border-radius:6px;\n",
    "               font-size:13px; font-weight:600;\">{len(ids)} tokens</span>\n",
    "        <span style=\"background:#e2e8f0; padding:3px 10px; border-radius:6px;\n",
    "               font-size:13px; font-weight:600;\">{len(text)} chars</span>\n",
    "        <span style=\"background:#fef3c7; padding:3px 10px; border-radius:6px;\n",
    "               font-size:13px; font-weight:600;\">ratio: {len(ids)/max(len(text),1):.2f} tok/char</span>\n",
    "      </div>\n",
    "      <div class=\"tv-label\">Tokenized Text</div>\n",
    "      <div class=\"tv-box\">{''.join(spans_html)}</div>\n",
    "      <div class=\"tv-label\" style=\"margin-top:10px;\">Token IDs</div>\n",
    "      <div class=\"tv-box\" style=\"line-height:2.2;\">{''.join(ids_html)}</div>\n",
    "    </div>\n",
    "    <script>\n",
    "    function hlTok(idx) {{\n",
    "      document.querySelectorAll('.tok,.tid').forEach(el => {{\n",
    "        if (idx >= 0 && el.dataset.i == idx) el.classList.add('hl');\n",
    "        else el.classList.remove('hl');\n",
    "      }});\n",
    "    }}\n",
    "    </script>\n",
    "    \"\"\"\n",
    "\n",
    "default_code = \"\"\"fn main() {\n",
    "    let mut v: Vec<i32> = Vec::new();\n",
    "    v.push(42);\n",
    "    println!(\"Hello, Rust! {}\", v[0]);\n",
    "}\"\"\"\n",
    "\n",
    "textarea = widgets.Textarea(\n",
    "    value=default_code, layout=widgets.Layout(width=\"100%\", height=\"180px\"),\n",
    "    description=\"Rust code:\", style={\"description_width\": \"80px\"},\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_change(change):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(render_tokens(change[\"new\"], tokenizer)))\n",
    "\n",
    "textarea.observe(on_change, names=\"value\")\n",
    "\n",
    "display(textarea, output)\n",
    "# Trigger initial render\n",
    "with output:\n",
    "    display(HTML(render_tokens(default_code, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mi2nr4vti4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8524acfd8a834d419a1d744792a080a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 10000 files, 19,895,335 tokens so far...\n",
      "Tokenized 20000 files, 39,470,086 tokens so far...\n",
      "Tokenized 30000 files, 59,801,287 tokens so far...\n",
      "Tokenized 40000 files, 77,448,586 tokens so far...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i >= MAX_FILES:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m ids = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.ids\n\u001b[32m     21\u001b[39m all_tokens.extend(ids)\n\u001b[32m     22\u001b[39m all_tokens.append(EOT_ID)  \u001b[38;5;66;03m# separate documents with EOT token\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Tokenize the full dataset and save as token IDs\n",
    "# Stream the dataset again and tokenize each file\n",
    "EOT_ID = tokenizer.token_to_id(\"<|endoftext|>\")\n",
    "\n",
    "tokenized_stream = load_dataset(\n",
    "    \"bigcode/the-stack-dedup\",\n",
    "    data_dir=\"data/rust\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "all_tokens = []\n",
    "MAX_FILES = 100_000  # limit for manageable size; increase as needed\n",
    "\n",
    "for i, sample in enumerate(tokenized_stream):\n",
    "    if i >= MAX_FILES:\n",
    "        break\n",
    "    ids = tokenizer.encode(sample[\"content\"]).ids\n",
    "    all_tokens.extend(ids)\n",
    "    all_tokens.append(EOT_ID)  # separate documents with EOT token\n",
    "    if (i + 1) % 10_000 == 0:\n",
    "        print(f\"Tokenized {i + 1} files, {len(all_tokens):,} tokens so far...\")\n",
    "\n",
    "all_tokens = np.array(all_tokens, dtype=np.uint16)\n",
    "print(f\"\\nTotal tokens: {len(all_tokens):,}\")\n",
    "print(f\"Array size: {all_tokens.nbytes / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pmhjabb75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val split and save\n",
    "split_idx = int(len(all_tokens) * 0.9)\n",
    "train_tokens = all_tokens[:split_idx]\n",
    "val_tokens = all_tokens[split_idx:]\n",
    "\n",
    "np.save(\"data/rust_train_tokens.npy\", train_tokens)\n",
    "np.save(\"data/rust_val_tokens.npy\", val_tokens)\n",
    "\n",
    "print(f\"Train tokens: {len(train_tokens):,}\")\n",
    "print(f\"Val tokens:   {len(val_tokens):,}\")\n",
    "print(\"Saved to data/rust_train_tokens.npy and data/rust_val_tokens.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
